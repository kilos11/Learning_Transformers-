{"cells":[{"cell_type":"markdown","metadata":{"id":"lj4eR9h22lA3"},"source":["#**Batch encoding**#\n","##The output of a tokenizer isn't a simple Python dictionary; what we get is actually a special `BatchEncoding` object. It's a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers.\n","\n","#√∑Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from -- a feature we call offset mapping. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it's inside, and vice versa."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["c8de639204964ab69f659e6624edf201","c2036a5db94d4dd592c77ad35d0b59c3","64d97bf07a894616827e8d7e43519e4a","f716a686e74448459f1fa6049ec49f3f","dc9ccd1925924b0181d22745e3d1169a","5a81044e13d449338103d331380a369b","0fa2e2c3985147aeadff2cc631053318","357fa11e2d89430bb87c83e33e9c04a1","0b3efdfd019644528206f96d3331fbae","5c0cb0cec42d4660aa4739512dc09bc9","38fcab9a31af493daefbfd66d63104f5","2b940d0a52d7424987f7871254a4f9a8","f6a4fa6e9e2a4bd09e7b57101d297fdd","10fd909cec394bb3b87292bf0d03b658","48473ee26bec4057b0a9f0236e5c7aa9","db61cdb50f28402eac69b42bf6fd3ae5","f9d6f2616d964104bb9bb1f0e4b26a3d","fb1700a903ba487f8dda9c78711568cd","6cf231941279434b849810034d4365da","53e1eb65298b404dbaf1140f1198f73f","c96e383debfd44cd9d76ae2fa534a6b4","c4104adb20ba4c28a823ad20bdaec00e","982f5891b75449e99f0b5e13682c0a00","ab0e3c61a3094cb390e2880ae0b23c6c","1cf5d1122728418982fa3a302124427c","27510ff1e13f4cb2861e12dbb1761040","66242473ba7349a29a1a69e9596b78b8","b6227d6c4ec84f02b7ec29faceeb18d5","bfef7b2f63054fd591855d4313bcc5a4","eec64656827942398f8c53012af20782","100caefe20af481b8937fbee3f3e2261","f78dda4e532348ddad86a6ce05832245","50ac56359ed84e1fb48d2d5b1df0ae92","1d1055fd37a0451c8af3aceaa2329249","91bd454cb97e4541a95945ab9a34b38b","ec5ccd1b8cc44d0b82f1baeabc7de965","da4866937a234773b82417b02d2284fd","947e0dae9669474ea6ae851675431ca2","6b5445ea5ff14252b0dd536f90f34324","ff335da8e25f4c618fe02bb60ad81ab0","e64542ba5fbf499faf9572d8e8f5a6d9","4f734cf3846b4b048fdb9424559446c6","c6b990598a7b4da79b1dca8b42c1b8fd","5ffe43cd30804739960e490aa7191966"],"height":339},"executionInfo":{"elapsed":8511,"status":"ok","timestamp":1720620698490,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"-Bz0gDoc3NrX","outputId":"eed6c041-b132-465a-8657-a3ad21755162"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8de639204964ab69f659e6624edf201"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b940d0a52d7424987f7871254a4f9a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982f5891b75449e99f0b5e13682c0a00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1055fd37a0451c8af3aceaa2329249"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'transformers.tokenization_utils_base.BatchEncoding'>\n","{'input_ids': [101, 100, 13665, 1122, 1149, 106, 140, 15998, 1240, 1319, 1859, 3087, 1105, 1267, 1191, 1128, 1169, 2437, 1134, 22559, 1116, 1132, 2628, 1114, 1937, 10999, 117, 1105, 1145, 1293, 1106, 16143, 1103, 1959, 15533, 1111, 170, 1423, 1937, 119, 1370, 6992, 1827, 117, 2222, 1606, 1160, 12043, 1112, 7758, 1105, 1267, 1191, 1103, 5650, 10999, 1116, 1294, 2305, 1106, 1128, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","example = \"‚úèÔ∏è Try it out! Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you.\"\n","encoding = tokenizer(example)\n","print(type(encoding))\n","print(encoding)"]},{"cell_type":"markdown","metadata":{"id":"j-iGQQn032Xn"},"source":["##Since the AutoTokenizer class picks a fast tokenizer by default, we can use the additional methods this `BatchEncoding` object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqZaakPb4VUj","outputId":"4a926e43-81c7-4c93-ab2b-fda8da1a9b36","executionInfo":{"status":"ok","timestamp":1720620842355,"user_tz":-120,"elapsed":475,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["tokenizer.is_fast"]},{"cell_type":"markdown","metadata":{"id":"VAUOelIH483t"},"source":["##check the same attribute of our `encoding`:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1720620846155,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"85h0DlDP5ItF","outputId":"40b0d625-839b-4411-ca44-1bbecb4c1244"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["encoding.is_fast"]},{"cell_type":"markdown","metadata":{"id":"WGLjJd9i5WVt"},"source":["#√∑Let's see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":709,"status":"ok","timestamp":1720535544920,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"Apjk8oMp5cr2","outputId":"3f637187-25a8-46f1-eec3-a1f698e0a1d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," '[UNK]',\n"," 'Try',\n"," 'it',\n"," 'out',\n"," '!',\n"," 'C',\n"," '##reate',\n"," 'your',\n"," 'own',\n"," 'example',\n"," 'text',\n"," 'and',\n"," 'see',\n"," 'if',\n"," 'you',\n"," 'can',\n"," 'understand',\n"," 'which',\n"," 'token',\n"," '##s',\n"," 'are',\n"," 'associated',\n"," 'with',\n"," 'word',\n"," 'ID',\n"," ',',\n"," 'and',\n"," 'also',\n"," 'how',\n"," 'to',\n"," 'extract',\n"," 'the',\n"," 'character',\n"," 'spans',\n"," 'for',\n"," 'a',\n"," 'single',\n"," 'word',\n"," '.',\n"," 'For',\n"," 'bonus',\n"," 'points',\n"," ',',\n"," 'try',\n"," 'using',\n"," 'two',\n"," 'sentences',\n"," 'as',\n"," 'input',\n"," 'and',\n"," 'see',\n"," 'if',\n"," 'the',\n"," 'sentence',\n"," 'ID',\n"," '##s',\n"," 'make',\n"," 'sense',\n"," 'to',\n"," 'you',\n"," '.',\n"," '[SEP]']"]},"metadata":{},"execution_count":4}],"source":["encoding.tokens()"]},{"cell_type":"markdown","metadata":{"id":"oYkpBHlZ5vE8"},"source":["##In this case the token at `index 5 is ##yl`, which is part of the word `\"Sylvain\"` in the original sentence. We can also use the `word_ids()` method to get the index of the word each token comes from:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1720620886130,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"Wir6Vhzs6Ixd","outputId":"051d0a81-778b-4edd-d8e1-b3279d7283e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[None,\n"," 0,\n"," 1,\n"," 2,\n"," 3,\n"," 4,\n"," 5,\n"," 5,\n"," 6,\n"," 7,\n"," 8,\n"," 9,\n"," 10,\n"," 11,\n"," 12,\n"," 13,\n"," 14,\n"," 15,\n"," 16,\n"," 17,\n"," 17,\n"," 18,\n"," 19,\n"," 20,\n"," 21,\n"," 22,\n"," 23,\n"," 24,\n"," 25,\n"," 26,\n"," 27,\n"," 28,\n"," 29,\n"," 30,\n"," 31,\n"," 32,\n"," 33,\n"," 34,\n"," 35,\n"," 36,\n"," 37,\n"," 38,\n"," 39,\n"," 40,\n"," 41,\n"," 42,\n"," 43,\n"," 44,\n"," 45,\n"," 46,\n"," 47,\n"," 48,\n"," 49,\n"," 50,\n"," 51,\n"," 52,\n"," 52,\n"," 53,\n"," 54,\n"," 55,\n"," 56,\n"," 57,\n"," None]"]},"metadata":{},"execution_count":4}],"source":["encoding.word_ids()"]},{"cell_type":"markdown","metadata":{"id":"oUddoy-661pC"},"source":["##We can see that the tokenizer's special tokens `[CLS]` and `[SEP]` are mapped to `None`, and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the `##` prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it's a fast one. In the next chapter, we'll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called whole word masking).\n","\n","##The notion of what a word is complicated. For instance, does \"I'll\" (a contraction of \"I will\") count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words.\n","\n","##Similarly, there is a `sentence_ids()` method that we can use to map a token to the sentence it came from (though in this case, the token_type_ids returned by the tokenizer can give us the same information).\n","\n","##Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the `word_ids()` method told us that `##yl` is part of the word at `index 31, but which word is it in the sentence? We can find out like this:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1720621091622,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"KX9kd_p68IQS","outputId":"58bb754e-d840-4103-c112-a7e5971e94bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'out'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["start, end = encoding.word_to_chars(3)\n","example[start:end]"]},{"cell_type":"markdown","metadata":{"id":"mI1OD0af9yus"},"source":["#**Inside the token-classification pipeline**#\n","##In Chapter 1 we got our first taste of applying NER -- where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations -- with the ü§ó Transformers `pipeline()` function. Then, in Chapter 2, we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: `tokenization, passing the inputs` through the model, and post-processing. The first two steps in the token-classification pipeline are the same as in any other pipeline, but the post-processing is a little more complex -- let's see how!"]},{"cell_type":"markdown","metadata":{"id":"UiYopBo9-eWl"},"source":["##**Getting the base results with the pipeline**##\n","##First, let's grab a token classification pipeline so we can get some results to compare manually. The model used by default is `dbmdz/bert-large-cased-finetuned-conll03-english`; it performs `NER` on sentences:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b4bb600198ff4545a2fd8d3a4e837261","1b4208bc0d83440ba35d882c53aca175","627d33304ff3413d84b9458833f55522","e7eb24e8ec714583853b807108ea0061","1998ace14a8e4d9db57af43fb52b3079","93daed2042ed4e54a807b8c5b083ae56","9697fc2c32434964911ea5f1d45f8140","9f3ea40b424f4db4ba7ca91d95a0430a","0cf0712863e84535998d0216a28154bb","f45d7d71e282434294f5b36072dce1bb","b678762efba9482c9845d12ee4638985","180372685cbf41959b3a406c33b169f5","c1a3b9bc29904623b3808485fbe706c2","401adfe9c22a4aaf8b0cb46918db0a0e","3968f338ba684d47b17349164a6f459c","e4da5d3757c34a32ba7dd1d3c7be53fa","88c9b3b509874e6d9bac5a300ddddb21","ed37e44684df4488808518cca71da97f","440699f72fd9478092ad77ed659ab2cb","723c6fac2579461ca82db7842924f338","a8c66e5f0ed94e44a4ff80d66950f467","61d5303f72d54cd29a7e15c7be1974a9","895bc8c6fa1f477fb6f34dbf847445ed","543792fbd78b4365b03b1c703f6a96ac","ace9cf2cab8944be91a24994bcd946a4","f2d85ef4b4a847999f4856a91d527493","6890995e55ef45dbaaefeffb2fcdc9e0","ddacec4b72ad436b93d1ab5165bad415","2f36fc516ed844f4a954a2638deacf12","9ab3d433a8764191aa49e795f5f7f88b","dd991dd329c745f8a2f1bc60e3054c45","2177083d346440e1917de27a06e0ff42","0b74298b34f2414682956d86b2b1b8bc","1cbbe6af7b1a469c9269480b74ca7d44","0a7e0bc6b4ea46af973d94aa9a2d6998","3bcd3eb13e5444b2a2b9ddf9047a7667","fd514717ef27409da1d932e3e4f257c0","d8b37948fe134a11b65f5efb1d44ab68","8596680c466d4349a52177d7a7bbc83f","c3093f03ffa84a709d56ef734dca6f93","987d4a8659bb4033ab6f060d0355dbc8","492e172699264d5c91affc5dc68d8530","addc8f8f722a40c3af01a7aeff78adbc","69ca2097f1b24da28512feaaf911d9f4"]},"executionInfo":{"elapsed":68293,"status":"ok","timestamp":1720621347938,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"y413wPu2_J9t","outputId":"2be5c5e5-dc6e-47d2-e802-f0838d154323"},"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4bb600198ff4545a2fd8d3a4e837261"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180372685cbf41959b3a406c33b169f5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895bc8c6fa1f477fb6f34dbf847445ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cbbe6af7b1a469c9269480b74ca7d44"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'I-PER',\n","  'score': 0.99938285,\n","  'index': 4,\n","  'word': 'S',\n","  'start': 11,\n","  'end': 12},\n"," {'entity': 'I-PER',\n","  'score': 0.99815494,\n","  'index': 5,\n","  'word': '##yl',\n","  'start': 12,\n","  'end': 14},\n"," {'entity': 'I-PER',\n","  'score': 0.99590707,\n","  'index': 6,\n","  'word': '##va',\n","  'start': 14,\n","  'end': 16},\n"," {'entity': 'I-PER',\n","  'score': 0.99923277,\n","  'index': 7,\n","  'word': '##in',\n","  'start': 16,\n","  'end': 18},\n"," {'entity': 'I-ORG',\n","  'score': 0.9738931,\n","  'index': 12,\n","  'word': 'Hu',\n","  'start': 33,\n","  'end': 35},\n"," {'entity': 'I-ORG',\n","  'score': 0.976115,\n","  'index': 13,\n","  'word': '##gging',\n","  'start': 35,\n","  'end': 40},\n"," {'entity': 'I-ORG',\n","  'score': 0.9887976,\n","  'index': 14,\n","  'word': 'Face',\n","  'start': 41,\n","  'end': 45},\n"," {'entity': 'I-LOC',\n","  'score': 0.9932106,\n","  'index': 16,\n","  'word': 'Brooklyn',\n","  'start': 49,\n","  'end': 57}]"]},"metadata":{},"execution_count":6}],"source":["from transformers import pipeline\n","\n","token_classifier = pipeline(\"token-classification\")\n","token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"]},{"cell_type":"markdown","metadata":{"id":"J_yjDb1X_vmu"},"source":["##The model properly identified each token generated by \"Sylvain\" as a person, each token generated by \"Hugging Face\" as an organization, and the token \"Brooklyn\" as a location. We can also ask the pipeline to group together the tokens that correspond to the same entity:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ve44H4XrAZJx","outputId":"bb144e54-8e95-48a6-bd77-ac2d472df69b","executionInfo":{"status":"ok","timestamp":1720621806068,"user_tz":-120,"elapsed":5075,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'entity_group': 'PER',\n","  'score': 0.9981694,\n","  'word': 'Sylvain',\n","  'start': 11,\n","  'end': 18},\n"," {'entity_group': 'ORG',\n","  'score': 0.9796019,\n","  'word': 'Hugging Face',\n","  'start': 33,\n","  'end': 45},\n"," {'entity_group': 'LOC',\n","  'score': 0.9932106,\n","  'word': 'Brooklyn',\n","  'start': 49,\n","  'end': 57}]"]},"metadata":{},"execution_count":7}],"source":["from transformers import pipeline\n","\n","token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n","token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"]},{"cell_type":"markdown","metadata":{"id":"MB09kjk5Cj14"},"source":["##The `aggregation_strategy` picked will change the scores computed for each grouped entity. With `\"simple\"` the score is just the mean of the scores of each token in the given entity: for instance, the score of `\"Sylvain\"` is the mean of the scores we saw in the previous example for the tokens `S, ##yl, ##va, and ##in`. Other strategies available are:\n","\n","##* `\"first\"`, where the score of each entity is the score of the first token of that entity (so for `\"Sylvain\"` it would be `0.993828, the score of the token S`)*##\n","\n","##*\"`max\"`, where the score of each entity is the maximum score of the tokens in that entity (so for `\"Hugging Face\" it would be 0.98879766, the score of \"Face\"`)*\n","##*\"`average\"`, where the score of each entity is the average of the scores of the words composing that entity (so for `\"Sylvain\" there would be no difference from the \"simple\" strategy, but \"Hugging Face\" would have a score of 0.9819, the average of the scores for \"Hugging\", 0.975, and \"Face\", 0.98879`)"]},{"cell_type":"markdown","metadata":{"id":"-fPEv1K-JPsm"},"source":["#**From inputs to predictions**#\n","\n","##First we need to tokenize our input and pass it through the model. This is done exactly as in Chapter 2; we instantiate the tokenizer and the model using the `AutoXxx` classes and then use them on our example:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1952,"status":"ok","timestamp":1720621815467,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"630dmqWQJtbb","outputId":"b63e0be0-d5b7-4828-fc45-5ba1091cdabf"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n","\n","example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n","inputs = tokenizer(example, return_tensors=\"pt\")\n","outputs = model(**inputs)"]},{"cell_type":"markdown","metadata":{"id":"RFJLjT2-_P33"},"source":["##Since we're using AutoModelForTokenClassification here, we get one set of logits for each token in the input sequence:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":541,"status":"ok","timestamp":1720621833862,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"GBkVkLQP_W1c","outputId":"1d7609ce-da68-4edc-f1f1-6a042a5fd4b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 19])\n","torch.Size([1, 19, 9])\n"]}],"source":["print(inputs[\"input_ids\"].shape)\n","print(outputs.logits.shape)"]},{"cell_type":"markdown","metadata":{"id":"lddlgAKeABaQ"},"source":["##First we need to tokenize our input and pass it through the model. This is done exactly as in Chapter 2; we instantiate the tokenizer and the model using the TFAutoXxx classes and then use them on our example:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15634,"status":"ok","timestamp":1720621863621,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"bdprCChmAI4R","outputId":"f3a5f597-81df-46ec-97d6-b69489f658d3"},"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForTokenClassification.\n","\n","All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"]}],"source":["from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n","\n","model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)\n","\n","example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n","inputs = tokenizer(example, return_tensors=\"tf\")\n","outputs = model(**inputs)"]},{"cell_type":"markdown","metadata":{"id":"HE7RbLwmAnay"},"source":["##Since we're using TFAutoModelForTokenClassification here, we get one set of logits for each token in the input sequence:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":486,"status":"ok","timestamp":1720621902798,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"nvHqi4e7AsfH","outputId":"2617f36c-b0b6-4d1b-fedd-f6d439c09650"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 19)\n","(1, 19, 9)\n"]}],"source":["print(inputs[\"input_ids\"].shape)\n","print(outputs.logits.shape)"]},{"cell_type":"markdown","metadata":{"id":"4WhtruMcAy2P"},"source":["##`We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9.` Like for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order):"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1720621956117,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"2Ch5oLLnD_F_","outputId":"3eb6ad01-1bf3-415c-e5d0-d04cb768c4d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n","[[0.9994322061538696, 1.6470285117975436e-05, 3.4267035516677424e-05, 1.6042313291109167e-05, 8.250691462308168e-05, 2.138227500836365e-05, 0.00015649088891223073, 1.9652095943456516e-05, 0.00022089220874477178], [0.9989631175994873, 1.851577326306142e-05, 5.240462633082643e-05, 1.253474511031527e-05, 0.00043473768164403737, 3.087438381044194e-05, 0.0003146878443658352, 2.7860780392074957e-05, 0.00014510893379338086], [0.9997084140777588, 8.308127689815592e-06, 2.874564415833447e-05, 5.6503645282646175e-06, 8.694856660440564e-05, 9.783467248780653e-06, 6.78614669595845e-05, 1.1794005331466906e-05, 7.241901039378718e-05], [0.9998350143432617, 5.645536930387607e-06, 1.3955179383629002e-05, 4.313381850806763e-06, 4.017698665848002e-05, 8.123078259814065e-06, 5.648501610266976e-05, 8.991642971523106e-06, 2.723914076341316e-05], [0.00018333389016333967, 2.5156619813060388e-05, 4.846203955821693e-05, 1.490058366471203e-05, 0.9993829131126404, 1.9997742128907703e-05, 0.0001115361083066091, 1.0790750820888206e-05, 0.00020288916130084544], [0.0006440267898142338, 7.437894237227738e-05, 0.00013196618237998337, 3.47196328220889e-05, 0.998154878616333, 3.382965223863721e-05, 0.0005438175867311656, 1.9978197087766603e-05, 0.00036244632792659104], [0.0016408368246629834, 9.469475480727851e-05, 0.0002736432943493128, 4.440633347257972e-05, 0.995907187461853, 5.126216638018377e-05, 0.0012787895975634456, 3.2835454476298764e-05, 0.0006763276760466397], [0.00022901821648702025, 2.518336805223953e-05, 5.789957140223123e-05, 9.957019756257068e-06, 0.9992326498031616, 1.7655071133049205e-05, 0.00023448366846423596, 1.2356603292573709e-05, 0.00018065073527395725], [0.9998047351837158, 5.46505316378898e-06, 1.295003039558651e-05, 4.9724485506885685e-06, 2.3503298507421277e-05, 1.2930753655382432e-05, 9.50994435697794e-05, 8.891789548215456e-06, 3.1469433452002704e-05], [0.9995046257972717, 1.4611845472245477e-05, 2.9646851544384845e-05, 8.223381882999092e-06, 0.0001601648546056822, 2.0456745914998464e-05, 0.00017537083476781845, 1.9349097783560865e-05, 6.743959966115654e-05], [0.9996776580810547, 7.596950126753654e-06, 1.7006959751597606e-05, 3.7776144381496124e-06, 6.396075332304463e-05, 1.2297868124733213e-05, 0.000182414471055381, 7.648396604054142e-06, 2.7664662411552854e-05], [0.9994348287582397, 1.1278340025455691e-05, 2.8862747058155946e-05, 6.2466042436426505e-06, 8.598280692240223e-05, 2.298943218193017e-05, 0.00034945266088470817, 1.384134066029219e-05, 4.6546654630219564e-05], [0.01815629191696644, 6.245922122616321e-05, 0.0002693223941605538, 4.7666246246080846e-05, 0.00613462645560503, 0.0002396748895989731, 0.9738930463790894, 7.093788735801354e-05, 0.0011259140446782112], [0.014645942486822605, 0.00020479779050219804, 0.0022360237780958414, 9.357067028759047e-05, 0.003731624921783805, 0.0005988662596791983, 0.9761150479316711, 0.00017609809583518654, 0.0021980921737849712], [0.0031715519726276398, 8.892122423276305e-05, 0.001500135869719088, 7.652968633919954e-05, 0.003357547102496028, 0.000464381038909778, 0.9887974858283997, 0.00010871275298995897, 0.0024345810525119305], [0.9995326399803162, 6.55397207083297e-06, 2.8316169846220873e-05, 6.235941327759065e-06, 3.7372072256403044e-05, 2.0357148969196714e-05, 0.00028727532480843365, 1.5032639566925354e-05, 6.614292215090245e-05], [0.0006589216645807028, 6.67124186293222e-05, 0.00022443967463914305, 4.190388790448196e-05, 0.0004602028930094093, 9.038775169756263e-05, 0.005088842008262873, 0.00015803190763108432, 0.9932106137275696], [0.9994322657585144, 1.647063072596211e-05, 3.42675884894561e-05, 1.6042420611483976e-05, 8.250880637206137e-05, 2.1382295017247088e-05, 0.00015649314445909113, 1.965226692846045e-05, 0.00022089369304012507], [0.9994322657585144, 1.6470268747070804e-05, 3.426700277486816e-05, 1.6042313291109167e-05, 8.250684186350554e-05, 2.138227500836365e-05, 0.00015649075794499367, 1.9652115952339955e-05, 0.00022089220874477178]]\n"]}],"source":["import tensorflow as tf  # Import TensorFlow library\n","\n","# Apply softmax function to logits to get probabilities\n","probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]\n","# Convert probabilities to a Python list\n","probabilities = probabilities.numpy().tolist()\n","\n","# Get the index of the highest probability (predicted class)\n","predictions = tf.math.argmax(outputs.logits, axis=-1)[0]\n"," # Convert predictions to a Python list\n","predictions = predictions.numpy().tolist()\n","\n","print(predictions)  # Print the predictions\n","print(probabilities)"]},{"cell_type":"markdown","metadata":{"id":"aBt4_DHAFp-4"},"source":["#√óThe `model.config.id2label` attribute contains the mapping of indexes to labels that we can use to make sense of the predictions:"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1720621979796,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"},"user_tz":-120},"id":"IfQf-X9MFz_5","outputId":"376518db-0f8a-4738-aaed-e61db8e27e8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O',\n"," 1: 'B-MISC',\n"," 2: 'I-MISC',\n"," 3: 'B-PER',\n"," 4: 'I-PER',\n"," 5: 'B-ORG',\n"," 6: 'I-ORG',\n"," 7: 'B-LOC',\n"," 8: 'I-LOC'}"]},"metadata":{},"execution_count":13}],"source":["model.config.id2label"]},{"cell_type":"markdown","metadata":{"id":"Sj-6nMCEGMGT"},"source":["##As we saw earlier, there are **`9 labels: O is the label for the tokens that are not in any named entity (it stands for \"outside\"), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label B-XXX indicates the token is at the beginning of an entity XXX and the label I-XXX indicates the token is inside the entity XXX. For instance, in the current example we would expect our model to classify the token S as B-PER (beginning of a person entity) and the tokens ##yl, ##va and ##in as I-PER (inside a person entity).`**\n","\n","**You might think the model was wrong in this case as it gave the label I-PER to all four of these tokens, but that's not entirely true. There are actually two formats for those B- and I- labels: IOB1 and IOB2. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with B- are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label I-PER to the S token.**"]},{"cell_type":"markdown","source":["##With this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we can just grab the score and label of each token that was not classified as O:"],"metadata":{"id":"F-5vFWyz7E9c"}},{"cell_type":"code","source":["# Initialize an empty list to store the final results\n","results = []\n","\n","# Tokenize the input text into individual tokens\n","tokens = inputs.tokens()\n","\n","# Iterate through the predictions and corresponding tokens\n","for idx, pred in enumerate(predictions):\n","    # Convert the numeric prediction to its corresponding label\n","    label = model.config.id2label[pred]\n","\n","    # Check if the label is not \"O\" (which typically means \"Outside\" or \"No entity\")\n","    if label != \"O\":\n","        # If it's a named entity, append a dictionary\n","        #with entity details to the results\n","        results.append({\n","            # The type of entity (e.g., \"PERSON\", \"LOCATION\")\n","            \"entity\": label,\n","            # The confidence score for this prediction\n","            \"score\": probabilities[idx][pred],\n","            # The actual word/token that was classified\n","            \"word\": tokens[idx]\n","        })\n","\n","# Print the final results list containing all identified entities\n","print(results)"],"metadata":{"id":"_QTD85if7MMw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720622437029,"user_tz":-120,"elapsed":519,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"31f65b17-a19f-4f17-d0e9-cb78285155d7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'entity': 'I-PER', 'score': 0.9993829131126404, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.998154878616333, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992326498031616, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738930463790894, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761150479316711, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}]\n"]}]},{"cell_type":"markdown","source":["##This is very similar to what we had before, with one exception: the pipeline also gave us information about the start and end of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs:"],"metadata":{"id":"jOBa_Wui-XiH"}},{"cell_type":"code","source":["inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n","inputs_with_offsets[\"offset_mapping\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Of6YEkJ-jON","executionInfo":{"status":"ok","timestamp":1720622579101,"user_tz":-120,"elapsed":416,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"d478e600-a642-4af7-fee2-c4d822ccc4bc"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0),\n"," (0, 2),\n"," (3, 7),\n"," (8, 10),\n"," (11, 12),\n"," (12, 14),\n"," (14, 16),\n"," (16, 18),\n"," (19, 22),\n"," (23, 24),\n"," (25, 29),\n"," (30, 32),\n"," (33, 35),\n"," (35, 40),\n"," (41, 45),\n"," (46, 48),\n"," (49, 57),\n"," (57, 58),\n"," (0, 0)]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["##Each tuple is the span of text corresponding to each token, where (0, 0) is reserved for the special tokens. We saw before that the token at index 5 is ##yl, which has (12, 14) as offsets here. If we grab the corresponding slice in our example:"],"metadata":{"id":"6ritjjD4_LIx"}},{"cell_type":"code","source":["example[12:14]"],"metadata":{"id":"bojZkbijvF2G","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1720622584934,"user_tz":-120,"elapsed":592,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"46f5fd54-9f4c-49d3-bc34-241a2bd7b106"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'yl'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["#**Using this, we can now complete the previous results:**#"],"metadata":{"id":"gFs3LFdCvUUP"}},{"cell_type":"code","source":["# We're making an empty list called 'results' to store our findings\n","results = []\n","\n","# We're using a special tool (tokenizer) to break down our example text into smaller pieces\n","# We're also asking it to remember where each piece starts and ends in the original text\n","inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n","\n","# We're getting all the individual words or parts of words from our text\n","tokens = inputs_with_offsets.tokens()\n","\n","# We're also getting the start and end positions of each word in the original text\n","offsets = inputs_with_offsets[\"offset_mapping\"]\n","\n","# Now we're going to look at each word one by one\n","for idx, pred in enumerate(predictions):\n","    # We're figuring out what kind of word this is (like if it's a name or a place)\n","    label = model.config.id2label[pred]\n","\n","    # If it's a special kind of word (not just a regular word)\n","    if label != \"O\":\n","        # We're finding out where this word starts and ends in the original text\n","        start, end = offsets[idx]\n","\n","        # We're adding information about this special word to our results list\n","        results.append({\n","            # What kind of special word is it?\n","            \"entity\": label,\n","            # How sure are we about this?\n","            \"score\": probabilities[idx][pred],\n","            # What's the actual word?\n","            \"word\": tokens[idx],\n","            # Where does it start in the original text?\n","            \"start\": start,\n","            #Where does it end in the original text?\n","            \"end\": end,\n","        })\n","\n","# Finally, we're showing all the special words we found\n","print(results)"],"metadata":{"id":"k36Kfvzuwdqx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720622609015,"user_tz":-120,"elapsed":495,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"4669bdab-79ae-4208-c7dc-3a04e235a01e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'entity': 'I-PER', 'score': 0.9993829131126404, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.998154878616333, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992326498031616, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738930463790894, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761150479316711, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"]}]},{"cell_type":"markdown","source":["#**Grouping entities**#\n","##Using the offsets to determine the start and end keys for each entity is handy, but that information isn't strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens `Hu, ##gging`, and Face, we could make special rules that say the first two should be attached while removing the ##, and the Face should be added with a space since it does not begin with ## -- but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter).\n","\n","##With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens Hu, ##gging, and Face, we should start at character 33 (the beginning of Hu) and end before character 45 (the end of Face):"],"metadata":{"id":"OlARim8Ay81x"}},{"cell_type":"code","source":["example[33:45]"],"metadata":{"id":"lCbzkJiTzYrQ","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1720622623820,"user_tz":-120,"elapsed":621,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"e8f4e5b6-d6d4-4a0d-85cc-24e5d601aed5"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hugging Face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["#√∑To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with I-XXX, except for the first one, which can be labeled as B-XXX or I-XXX (so, we stop grouping an entity when we get a O, a new type of entity, or a B-XXX that tells us an entity of the same type is starting):"],"metadata":{"id":"yZsekPcwzkQq"}},{"cell_type":"code","source":["import numpy as np  # We're bringing in a special tool called numpy to help us with math\n","\n","results = []  # We're making an empty list to store our findings\n","\n","# We're using a special tool to break our example text into smaller pieces\n","# and remember where each piece starts and ends\n","inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n","\n","# We're getting all the individual words or parts of words\n","tokens = inputs_with_offsets.tokens()\n","\n","# We're getting the start and end positions of each word\n","offsets = inputs_with_offsets[\"offset_mapping\"]\n","\n","idx = 0  # We're starting a counter at 0\n","\n","# We're going to look at each word one by one until we've seen them all\n","while idx < len(predictions):\n","    # We're looking at the current prediction\n","    pred = predictions[idx]\n","    # We're figuring out what kind of word this is\n","    label = model.config.id2label[pred]\n","\n","    # If it's a special kind of word (not just a regular word)\n","    if label != \"O\":\n","        # We're removing the first two letters (like \"B-\" or \"I-\")\n","        label = label[2:]\n","        # We're finding out where this word starts\n","        start, _ = offsets[idx]\n","\n","        # We're making a new list to keep track of how sure we are about each part\n","        all_scores = []\n","\n","        # We're going to keep looking at words as long as they're part of the same\n","        #special group\n","        while (\n","            idx < len(predictions)\n","            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n","        ):\n","            # We're remembering how sure we are\n","            all_scores.append(probabilities[idx][pred])\n","            # We're updating where the word ends\n","            _, end = offsets[idx]\n","            # We're moving to the next word\n","            idx += 1\n","\n","        # We're calculating how sure we are overall by taking\n","        #the average of all our scores\n","        score = np.mean(all_scores).item()\n","\n","        # We're getting the actual word or group of words\n","        #from our example\n","        word = example[start:end]\n","\n","        # We're adding all the information we found to our results list\n","        results.append({\n","            # What kind of special word or group is it?\n","            \"entity_group\": label,\n","            # How sure are we about it?\n","            \"score\": score,\n","            # What's the actual word or group of words?\n","            \"word\": word,\n","            # Where does it start in the original text?\n","            \"start\": start,\n","            # Where does it end in the original text?\n","            \"end\": end,\n","        })\n","\n","    idx += 1  # We're moving to the next word\n","\n","# Finally, we're showing all the special words or groups we found\n","print(results)\n","\n"],"metadata":{"id":"zScKb7t30zPS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720622676070,"user_tz":-120,"elapsed":444,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"a4643fff-1432-4a7a-dc9a-de3a96335f71"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'entity_group': 'PER', 'score': 0.998169407248497, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796018600463867, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMC41vYY92/VkptonQKfHcM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c8de639204964ab69f659e6624edf201":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2036a5db94d4dd592c77ad35d0b59c3","IPY_MODEL_64d97bf07a894616827e8d7e43519e4a","IPY_MODEL_f716a686e74448459f1fa6049ec49f3f"],"layout":"IPY_MODEL_dc9ccd1925924b0181d22745e3d1169a"}},"c2036a5db94d4dd592c77ad35d0b59c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a81044e13d449338103d331380a369b","placeholder":"‚Äã","style":"IPY_MODEL_0fa2e2c3985147aeadff2cc631053318","value":"tokenizer_config.json:‚Äá100%"}},"64d97bf07a894616827e8d7e43519e4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_357fa11e2d89430bb87c83e33e9c04a1","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b3efdfd019644528206f96d3331fbae","value":49}},"f716a686e74448459f1fa6049ec49f3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0cb0cec42d4660aa4739512dc09bc9","placeholder":"‚Äã","style":"IPY_MODEL_38fcab9a31af493daefbfd66d63104f5","value":"‚Äá49.0/49.0‚Äá[00:00&lt;00:00,‚Äá2.06kB/s]"}},"dc9ccd1925924b0181d22745e3d1169a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a81044e13d449338103d331380a369b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fa2e2c3985147aeadff2cc631053318":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"357fa11e2d89430bb87c83e33e9c04a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b3efdfd019644528206f96d3331fbae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c0cb0cec42d4660aa4739512dc09bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38fcab9a31af493daefbfd66d63104f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b940d0a52d7424987f7871254a4f9a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6a4fa6e9e2a4bd09e7b57101d297fdd","IPY_MODEL_10fd909cec394bb3b87292bf0d03b658","IPY_MODEL_48473ee26bec4057b0a9f0236e5c7aa9"],"layout":"IPY_MODEL_db61cdb50f28402eac69b42bf6fd3ae5"}},"f6a4fa6e9e2a4bd09e7b57101d297fdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9d6f2616d964104bb9bb1f0e4b26a3d","placeholder":"‚Äã","style":"IPY_MODEL_fb1700a903ba487f8dda9c78711568cd","value":"config.json:‚Äá100%"}},"10fd909cec394bb3b87292bf0d03b658":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf231941279434b849810034d4365da","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53e1eb65298b404dbaf1140f1198f73f","value":570}},"48473ee26bec4057b0a9f0236e5c7aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c96e383debfd44cd9d76ae2fa534a6b4","placeholder":"‚Äã","style":"IPY_MODEL_c4104adb20ba4c28a823ad20bdaec00e","value":"‚Äá570/570‚Äá[00:00&lt;00:00,‚Äá24.9kB/s]"}},"db61cdb50f28402eac69b42bf6fd3ae5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9d6f2616d964104bb9bb1f0e4b26a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb1700a903ba487f8dda9c78711568cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cf231941279434b849810034d4365da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53e1eb65298b404dbaf1140f1198f73f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c96e383debfd44cd9d76ae2fa534a6b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4104adb20ba4c28a823ad20bdaec00e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982f5891b75449e99f0b5e13682c0a00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab0e3c61a3094cb390e2880ae0b23c6c","IPY_MODEL_1cf5d1122728418982fa3a302124427c","IPY_MODEL_27510ff1e13f4cb2861e12dbb1761040"],"layout":"IPY_MODEL_66242473ba7349a29a1a69e9596b78b8"}},"ab0e3c61a3094cb390e2880ae0b23c6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6227d6c4ec84f02b7ec29faceeb18d5","placeholder":"‚Äã","style":"IPY_MODEL_bfef7b2f63054fd591855d4313bcc5a4","value":"vocab.txt:‚Äá100%"}},"1cf5d1122728418982fa3a302124427c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eec64656827942398f8c53012af20782","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_100caefe20af481b8937fbee3f3e2261","value":213450}},"27510ff1e13f4cb2861e12dbb1761040":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f78dda4e532348ddad86a6ce05832245","placeholder":"‚Äã","style":"IPY_MODEL_50ac56359ed84e1fb48d2d5b1df0ae92","value":"‚Äá213k/213k‚Äá[00:00&lt;00:00,‚Äá3.32MB/s]"}},"66242473ba7349a29a1a69e9596b78b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6227d6c4ec84f02b7ec29faceeb18d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfef7b2f63054fd591855d4313bcc5a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eec64656827942398f8c53012af20782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100caefe20af481b8937fbee3f3e2261":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f78dda4e532348ddad86a6ce05832245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50ac56359ed84e1fb48d2d5b1df0ae92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d1055fd37a0451c8af3aceaa2329249":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91bd454cb97e4541a95945ab9a34b38b","IPY_MODEL_ec5ccd1b8cc44d0b82f1baeabc7de965","IPY_MODEL_da4866937a234773b82417b02d2284fd"],"layout":"IPY_MODEL_947e0dae9669474ea6ae851675431ca2"}},"91bd454cb97e4541a95945ab9a34b38b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b5445ea5ff14252b0dd536f90f34324","placeholder":"‚Äã","style":"IPY_MODEL_ff335da8e25f4c618fe02bb60ad81ab0","value":"tokenizer.json:‚Äá100%"}},"ec5ccd1b8cc44d0b82f1baeabc7de965":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e64542ba5fbf499faf9572d8e8f5a6d9","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f734cf3846b4b048fdb9424559446c6","value":435797}},"da4866937a234773b82417b02d2284fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b990598a7b4da79b1dca8b42c1b8fd","placeholder":"‚Äã","style":"IPY_MODEL_5ffe43cd30804739960e490aa7191966","value":"‚Äá436k/436k‚Äá[00:00&lt;00:00,‚Äá11.2MB/s]"}},"947e0dae9669474ea6ae851675431ca2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b5445ea5ff14252b0dd536f90f34324":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff335da8e25f4c618fe02bb60ad81ab0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e64542ba5fbf499faf9572d8e8f5a6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f734cf3846b4b048fdb9424559446c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6b990598a7b4da79b1dca8b42c1b8fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ffe43cd30804739960e490aa7191966":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4bb600198ff4545a2fd8d3a4e837261":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b4208bc0d83440ba35d882c53aca175","IPY_MODEL_627d33304ff3413d84b9458833f55522","IPY_MODEL_e7eb24e8ec714583853b807108ea0061"],"layout":"IPY_MODEL_1998ace14a8e4d9db57af43fb52b3079"}},"1b4208bc0d83440ba35d882c53aca175":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93daed2042ed4e54a807b8c5b083ae56","placeholder":"‚Äã","style":"IPY_MODEL_9697fc2c32434964911ea5f1d45f8140","value":"config.json:‚Äá100%"}},"627d33304ff3413d84b9458833f55522":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f3ea40b424f4db4ba7ca91d95a0430a","max":998,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0cf0712863e84535998d0216a28154bb","value":998}},"e7eb24e8ec714583853b807108ea0061":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f45d7d71e282434294f5b36072dce1bb","placeholder":"‚Äã","style":"IPY_MODEL_b678762efba9482c9845d12ee4638985","value":"‚Äá998/998‚Äá[00:00&lt;00:00,‚Äá47.9kB/s]"}},"1998ace14a8e4d9db57af43fb52b3079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93daed2042ed4e54a807b8c5b083ae56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9697fc2c32434964911ea5f1d45f8140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f3ea40b424f4db4ba7ca91d95a0430a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cf0712863e84535998d0216a28154bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f45d7d71e282434294f5b36072dce1bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b678762efba9482c9845d12ee4638985":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"180372685cbf41959b3a406c33b169f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1a3b9bc29904623b3808485fbe706c2","IPY_MODEL_401adfe9c22a4aaf8b0cb46918db0a0e","IPY_MODEL_3968f338ba684d47b17349164a6f459c"],"layout":"IPY_MODEL_e4da5d3757c34a32ba7dd1d3c7be53fa"}},"c1a3b9bc29904623b3808485fbe706c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88c9b3b509874e6d9bac5a300ddddb21","placeholder":"‚Äã","style":"IPY_MODEL_ed37e44684df4488808518cca71da97f","value":"model.safetensors:‚Äá100%"}},"401adfe9c22a4aaf8b0cb46918db0a0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_440699f72fd9478092ad77ed659ab2cb","max":1334400964,"min":0,"orientation":"horizontal","style":"IPY_MODEL_723c6fac2579461ca82db7842924f338","value":1334400964}},"3968f338ba684d47b17349164a6f459c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8c66e5f0ed94e44a4ff80d66950f467","placeholder":"‚Äã","style":"IPY_MODEL_61d5303f72d54cd29a7e15c7be1974a9","value":"‚Äá1.33G/1.33G‚Äá[00:53&lt;00:00,‚Äá26.8MB/s]"}},"e4da5d3757c34a32ba7dd1d3c7be53fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88c9b3b509874e6d9bac5a300ddddb21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed37e44684df4488808518cca71da97f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"440699f72fd9478092ad77ed659ab2cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"723c6fac2579461ca82db7842924f338":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8c66e5f0ed94e44a4ff80d66950f467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61d5303f72d54cd29a7e15c7be1974a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"895bc8c6fa1f477fb6f34dbf847445ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_543792fbd78b4365b03b1c703f6a96ac","IPY_MODEL_ace9cf2cab8944be91a24994bcd946a4","IPY_MODEL_f2d85ef4b4a847999f4856a91d527493"],"layout":"IPY_MODEL_6890995e55ef45dbaaefeffb2fcdc9e0"}},"543792fbd78b4365b03b1c703f6a96ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddacec4b72ad436b93d1ab5165bad415","placeholder":"‚Äã","style":"IPY_MODEL_2f36fc516ed844f4a954a2638deacf12","value":"tokenizer_config.json:‚Äá100%"}},"ace9cf2cab8944be91a24994bcd946a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ab3d433a8764191aa49e795f5f7f88b","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd991dd329c745f8a2f1bc60e3054c45","value":60}},"f2d85ef4b4a847999f4856a91d527493":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2177083d346440e1917de27a06e0ff42","placeholder":"‚Äã","style":"IPY_MODEL_0b74298b34f2414682956d86b2b1b8bc","value":"‚Äá60.0/60.0‚Äá[00:00&lt;00:00,‚Äá1.62kB/s]"}},"6890995e55ef45dbaaefeffb2fcdc9e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddacec4b72ad436b93d1ab5165bad415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f36fc516ed844f4a954a2638deacf12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ab3d433a8764191aa49e795f5f7f88b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd991dd329c745f8a2f1bc60e3054c45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2177083d346440e1917de27a06e0ff42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b74298b34f2414682956d86b2b1b8bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cbbe6af7b1a469c9269480b74ca7d44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a7e0bc6b4ea46af973d94aa9a2d6998","IPY_MODEL_3bcd3eb13e5444b2a2b9ddf9047a7667","IPY_MODEL_fd514717ef27409da1d932e3e4f257c0"],"layout":"IPY_MODEL_d8b37948fe134a11b65f5efb1d44ab68"}},"0a7e0bc6b4ea46af973d94aa9a2d6998":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8596680c466d4349a52177d7a7bbc83f","placeholder":"‚Äã","style":"IPY_MODEL_c3093f03ffa84a709d56ef734dca6f93","value":"vocab.txt:‚Äá100%"}},"3bcd3eb13e5444b2a2b9ddf9047a7667":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_987d4a8659bb4033ab6f060d0355dbc8","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_492e172699264d5c91affc5dc68d8530","value":213450}},"fd514717ef27409da1d932e3e4f257c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_addc8f8f722a40c3af01a7aeff78adbc","placeholder":"‚Äã","style":"IPY_MODEL_69ca2097f1b24da28512feaaf911d9f4","value":"‚Äá213k/213k‚Äá[00:00&lt;00:00,‚Äá2.92MB/s]"}},"d8b37948fe134a11b65f5efb1d44ab68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8596680c466d4349a52177d7a7bbc83f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3093f03ffa84a709d56ef734dca6f93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"987d4a8659bb4033ab6f060d0355dbc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492e172699264d5c91affc5dc68d8530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"addc8f8f722a40c3af01a7aeff78adbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69ca2097f1b24da28512feaaf911d9f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}