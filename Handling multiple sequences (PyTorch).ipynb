{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7bGMGslYj4vEqV2mVv3sp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"1sjY4smHZ4ST","executionInfo":{"status":"ok","timestamp":1718866184838,"user_tz":-120,"elapsed":22126,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"90d705b8-4994-4990-bdbe-be7ed054175e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]}],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"markdown","source":["##The code starts by importing the necessary modules: `torch` for tensor operations and `AutoTokenizer` and `AutoModelForSequenceClassification` from the Transformers library.\n","\n","##Next, a pre-trained model and tokenizer for sentiment analysis are loaded using the `from_pretrained` method. In this case, the `\"distilbert-base-uncased-finetuned-sst-2-english\"` checkpoint is used, which is a DistilBERT model fine-tuned on the Stanford Sentiment Treebank (SST-2) dataset for binary sentiment classification.\n","\n","##The input sequence to be classified is defined as \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","##The input sequence is then tokenized using the `tokenizer.tokenize` method, which splits the sequence into individual tokens. The tokens are converted to their corresponding IDs using `tokenizer.convert_tokens_to_ids`, and the resulting IDs are converted to a PyTorch tensor using `torch.tensor`.\n","\n","##Finally, an attempt is made to pass the `input_ids` tensor to the model using `model(input_ids)`. However, this line will fail because the model expects additional input tensors, such as attention masks and token type IDs, which are not provided in this code.\n","\n","##To successfully run the model for sequence classification, you would need to provide the required input tensors. The Transformers library provides convenience functions like `tokenizer.encode` or `tokenizer.encode_plus` to handle the tokenization and tensor preparation steps in a single call.\n","\n","##This code demonstrates the basic steps involved in using the Transformers library for sequence classification tasks, including loading pre-trained models and tokenizers, tokenizing input sequences, and preparing input tensors for the model. However, it is important to note that additional steps may be required to properly handle the input data and obtain the desired output from the model."],"metadata":{"id":"45aCEDJkbXxA"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Load a pre-trained model and tokenizer for sentiment analysis\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","# Define the input sequence\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","# Tokenize the input sequence\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor(ids)\n","\n","# Attempt to pass the input_ids to the model (this line will fail)\n","model(input_ids)"],"metadata":{"id":"emkKyvMAay9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf6ZEGSUcEHK","executionInfo":{"status":"ok","timestamp":1718866695785,"user_tz":-120,"elapsed":501,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"ee4003f1-ce84-4520-8bae-bd730b55f736"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102]])\n"]}]},{"cell_type":"markdown","source":["##The code starts by importing the necessary modules: `torch` for tensor operations and `AutoTokenizer` and `AutoModelForSequenceClassification` from the Transformers library.\n","\n","##Next, a pre-trained DistilBERT model and tokenizer fine-tuned for sentiment analysis are loaded using the `from_pretrained` method.\n","\n","##The input sequence to be classified is defined as \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","##The input sequence is tokenized using the `tokenizer.tokenize` method, and the tokens are converted to their corresponding IDs using `tokenizer.convert_tokens_to_ids`. The resulting IDs are converted to a PyTorch tensor using `torch.tensor` and wrapped in a list to match the expected input format for the model.\n","\n","##The `input_ids` tensor is printed to the console for reference.\n","\n","##Finally, the `input_ids` tensor is passed to the model using `model(input_ids)`, and the output logits are obtained. The logits represent the raw scores for each class before applying the softmax function. In this case, since the model is fine-tuned for binary sentiment classification, there will be two logits: one for the positive sentiment class and one for the negative sentiment class.\n","\n","##The output logits are printed to the console.\n","\n","##To interpret the output and determine the predicted sentiment, you would need to apply the softmax function to the logits and select the class with the highest probability. Alternatively, you can use the built-in `model.forward` method, which applies the softmax function and returns the class probabilities directly."],"metadata":{"id":"3Q8KqvmLdghX"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Load a pre-trained model and tokenizer for sentiment analysis\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","# Define the input sequence\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","# Tokenize the input sequence\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","# Convert token IDs to a PyTorch tensor\n","input_ids = torch.tensor([ids])\n","print(\"Input IDs:\", input_ids)\n","\n","# Pass the input_ids to the model and get the output logits\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPziOxUWdAXZ","executionInfo":{"status":"ok","timestamp":1718867210260,"user_tz":-120,"elapsed":1365,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"d6cad680-5209-419e-c029-f943897453c4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012]])\n","Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["batched_ids = [\n","        [200, 200, 200],\n","        [200, 200]\n","            ]\n"],"metadata":{"id":"EuQVU4rTePNM","executionInfo":{"status":"ok","timestamp":1718867273585,"user_tz":-120,"elapsed":511,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["padding_id = 100\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, padding_id],\n","        ]"],"metadata":{"id":"D12NM3oveYcK","executionInfo":{"status":"ok","timestamp":1718867308356,"user_tz":-120,"elapsed":557,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["##The code starts by loading the pre-trained DistilBERT model for sequence classification using the `AutoModelForSequenceClassification.from_pretrained` method.\n","\n","##Next, three different input sequences are defined:\n","\n","##*1. `sequence1_ids`: A sequence of length 3, represented as a list of token IDs.*\n","##*2. `sequence2_ids`: A sequence of length 2, represented as a list of token IDs.*\n","##*3. `batched_ids`: A batch of two sequences, where the second sequence is padded to the same length as the first sequence using the `tokenizer.pad_token_id`.*\n","\n","##The padding is necessary because most models expect input sequences of the same length within a batch. The `tokenizer.pad_token_id` is a special token ID used to pad shorter sequences to the desired length.\n","\n","##Finally, each input sequence (or batch of sequences) is passed to the model using `model(torch.tensor(sequence_ids))`, and the output logits are printed to the console.\n","\n","##The output logits represent the raw scores for each class before applying the softmax function. In the case of binary classification, there will be two logits: one for the positive class and one for the negative class.\n","\n","##By using different input formats (single sequence and batched sequences with padding), this code demonstrates how to handle variable-length sequences and batches when using pre-trained models for sequence classification tasks."],"metadata":{"id":"EUzl2pkAgvYr"}},{"cell_type":"code","source":["# Load the pre-trained DistilBERT model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","# Define input sequences with different lengths\n","sequence1_ids = [[200, 200, 200]]  # Sequence of length 3\n","sequence2_ids = [[200, 200]]  # Sequence of length 2\n","batched_ids = [\n","        [200, 200, 200],  # Sequence of length 3\n","        [200, 200, tokenizer.pad_token_id],  # Sequence of length 2, padded to length 3\n","            ]\n","# Pass the input sequences to the model and print the output logits\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NI-XuAykgB1F","executionInfo":{"status":"ok","timestamp":1718868130615,"user_tz":-120,"elapsed":1236,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"52a8f665-35fe-4d80-e8f7-f40daca17fe5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","tensor([[ 1.5694, -1.3895],\n","        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"]}]}]}